<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Illarion Khlestov Blog</title><link>https://ikhlestov.github.io/</link><description>Site about coding notes</description><atom:link href="https://ikhlestov.github.io/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Fri, 30 Dec 2016 18:36:46 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>RBM based Autoencoders with tensorflow</title><link>https://ikhlestov.github.io/posts/rbm-based-autoencoders-with-tensorflow/</link><dc:creator>Illarion Khlestov</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;This is draft of post - feel free to comment/mail me about any errors&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Recently I try to implement RBM based autoencoder in tensorflow similar to RBMs described in &lt;a class="reference external" href="http://www.cs.utoronto.ca/~rsalakhu/papers/semantic_final.pdf"&gt;Semantic Hashing&lt;/a&gt; paper by Ruslan Salakhutdinov and Geoffrey Hinton. It seems that with RBM pretrained weights autoencoders should converge faster. So I've decide to check this.&lt;/p&gt;
&lt;p&gt;This post will describe some roadblocks for RBMs/autoencoders implementation in tensorflow and compare results of different approaches. I assume reader previous knowledge of tensorflow and machine learning field. All code can be found in &lt;a class="reference external" href="https://github.com/ikhlestov/rbm_based_autoencoders_with_tensorflow"&gt;this repo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;RBMs different from usual neural networks in some ways:&lt;/p&gt;
&lt;p&gt;Neural networks usually perform weight update by Gradient Descent, but RMBs use &lt;strong&gt;Contrastive Divergence&lt;/strong&gt; (which is basically a funky term for "approximate gradient descent" &lt;a class="reference external" href="http://deeplearning.net/tutorial/rbm.html"&gt;link to read&lt;/a&gt;). At a glance contrastive divergence compute difference between &lt;strong&gt;positive phase&lt;/strong&gt; (energy of first encoding) and &lt;strong&gt;negative phase&lt;/strong&gt; (energy of last encoding).&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_0af87689d84c438c8fde675e34acd902-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_0af87689d84c438c8fde675e34acd902-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;visib_inputs_initial&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;first_encoded_probability&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_0af87689d84c438c8fde675e34acd902-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;negative_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_0af87689d84c438c8fde675e34acd902-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;last_reconstructed_probability&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;last_encoded_probability&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_0af87689d84c438c8fde675e34acd902-5"&gt;&lt;/a&gt;&lt;span class="n"&gt;contrastive_divergence&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;negative_phase&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Also key feature of RMB that it encode output in binary mode, not as probabilities. More about RMBs you may read &lt;a class="reference external" href="http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/"&gt;here&lt;/a&gt; or &lt;a class="reference external" href="http://rocknrollnerd.github.io/ml/2015/07/18/general-boltzmann-machines.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As prototype one layer tensorflow rbm &lt;a class="reference external" href="https://github.com/blackecho/Deep-Learning-TensorFlow/blob/master/yadlt/models/rbm_models/rbm.py"&gt;implementation&lt;/a&gt; was used. For testing I've take usual &lt;a class="reference external" href="https://en.wikipedia.org/wiki/MNIST_database"&gt;MNIST&lt;/a&gt; dataset(dataset of handwritten digits).&lt;/p&gt;
&lt;div class="section" id="many-layers-implementation"&gt;
&lt;h2&gt;Many layers implementation&lt;/h2&gt;
&lt;p&gt;At first I've implement &lt;a class="reference external" href="https://github.com/ikhlestov/rbm_based_autoencoders_with_tensorflow/blob/master/rbm_all_layers_at_once.py"&gt;multi layers RBM&lt;/a&gt; with 3 layers. Because we not use usual tensorflow optimizers we may stop gradient for every variable with &lt;cite&gt;tf.stop_gradient(variable_name)&lt;/cite&gt; and this will speed up computation a little bit. After construction two questions arised:
- Should every layer hidden units be binary encoded or only last one?
- Should we update every layer weights/biases at once per step, or first train only first two layers, after layers 2 and 3, and so on?&lt;/p&gt;
&lt;p&gt;So I've run model with all binary units and only with last binary unit. And it seems that model with only last layer binarized trains better. After a while I note that this approach was already proposed in the paper, but I somehow miss this.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;a class="reference external image-reference" href="https://ikhlestov.github.io/images/rbm-based-autoencoders-with-tensorflow/01_layers_binarization.png"&gt;&lt;img alt="/images/rbm-based-autoencoders-with-tensorflow/01_layers_binarization.thumbnail.png" src="https://ikhlestov.github.io/images/rbm-based-autoencoders-with-tensorflow/01_layers_binarization.thumbnail.png"&gt;&lt;/a&gt;
&lt;p class="caption"&gt;Errors with respect to steps&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;So let's stop with last layer binarized and try different train approaches. To build model that will train only pair of layers we need train two layers model, save it, build new model with one more layer, load pretrained first two layers weights/biases and continue train last two layers (&lt;a class="reference external" href="https://github.com/ikhlestov/rbm_based_autoencoders_with_tensorflow/blob/master/rbm_train_by_pair_layers.py"&gt;code&lt;/a&gt;. During implementation I've meet some trouble - tensorflow have no method to initialize all not initialized previously variables method. Maybe I just didn't find this. So I've finish with approach when I directly send variable that should be restored and variables that should be initialized.&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_deaedd4c789245c187b62efaef3b1bcf-1"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# restore previous variables&lt;/span&gt;
&lt;a name="rest_code_deaedd4c789245c187b62efaef3b1bcf-2"&gt;&lt;/a&gt;&lt;span class="n"&gt;restore_vars_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_get_restored_variables_names&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_deaedd4c789245c187b62efaef3b1bcf-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;restorer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Saver&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;restore_vars_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_deaedd4c789245c187b62efaef3b1bcf-4"&gt;&lt;/a&gt;&lt;span class="n"&gt;restorer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;restore&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;saves_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_deaedd4c789245c187b62efaef3b1bcf-5"&gt;&lt;/a&gt;
&lt;a name="rest_code_deaedd4c789245c187b62efaef3b1bcf-6"&gt;&lt;/a&gt;&lt;span class="c1"&gt;# initialize not restored variables&lt;/span&gt;
&lt;a name="rest_code_deaedd4c789245c187b62efaef3b1bcf-7"&gt;&lt;/a&gt;&lt;span class="n"&gt;new_variables&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_get_new_variables_names&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;a name="rest_code_deaedd4c789245c187b62efaef3b1bcf-8"&gt;&lt;/a&gt;&lt;span class="n"&gt;sess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;initialize_variables&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_variables&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;After testing is seems that both training approaches converge to approximately same error. But some another cool stuff - model that was trained by pair lairs trains faster in time.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;a class="reference external image-reference" href="https://ikhlestov.github.io/images/rbm-based-autoencoders-with-tensorflow/02_layers_training_error.png"&gt;&lt;img alt="/images/rbm-based-autoencoders-with-tensorflow/02_layers_training_error.thumbnail.png" src="https://ikhlestov.github.io/images/rbm-based-autoencoders-with-tensorflow/02_layers_training_error.thumbnail.png"&gt;&lt;/a&gt;
&lt;p class="caption"&gt;Errors with respect to steps&lt;/p&gt;
&lt;/div&gt;
&lt;div class="figure"&gt;
&lt;a class="reference external image-reference" href="https://ikhlestov.github.io/images/rbm-based-autoencoders-with-tensorflow/02_layers_training_relative.png"&gt;&lt;img alt="/images/rbm-based-autoencoders-with-tensorflow/02_layers_training_relative.thumbnail.png" src="https://ikhlestov.github.io/images/rbm-based-autoencoders-with-tensorflow/02_layers_training_relative.thumbnail.png"&gt;&lt;/a&gt;
&lt;p class="caption"&gt;Errors with respect to time consumption&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;So we stop with RBM trained with only last layer binarized and trained with &lt;em&gt;two layers only&lt;/em&gt; strategy.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="build-autoencoder-from-rbm"&gt;
&lt;h2&gt;Build autoencoder from RBM&lt;/h2&gt;
&lt;p&gt;After get pretrained weights from RMB it's time to build autoencoder for fine tuning. To get encoding layer output as much as possible binarized as per paper advise we add Gaussian noise prior to layer. To simulate &lt;em&gt;deterministic noise&lt;/em&gt; behavior, noise was generate for each input prior training and not changed during training. Also we want compare autoencoder loaded from RBM weights with self initialized usual autoencoder. &lt;a class="reference external" href="https://github.com/ikhlestov/rbm_based_autoencoders_with_tensorflow/blob/master/autoencoder.py"&gt;Code for autoencoder&lt;/a&gt;&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;a class="reference external image-reference" href="https://ikhlestov.github.io/images/rbm-based-autoencoders-with-tensorflow/03_rbm_and_new_initialized_autoencoders.png"&gt;&lt;img alt="/images/rbm-based-autoencoders-with-tensorflow/03_rbm_and_new_initialized_autoencoders.thumbnail.png" src="https://ikhlestov.github.io/images/rbm-based-autoencoders-with-tensorflow/03_rbm_and_new_initialized_autoencoders.thumbnail.png"&gt;&lt;/a&gt;
&lt;p class="caption"&gt;RBM initialized autoencoder vs newly initialized autoencoder&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;It seems that RBM initialized autoencoder continue training, but newly initialized autoencoder with same architecture after a while stuck at some point.&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;a class="reference external image-reference" href="https://ikhlestov.github.io/images/rbm-based-autoencoders-with-tensorflow/03_rbm_initialized_autoencoder.png"&gt;&lt;img alt="/images/rbm-based-autoencoders-with-tensorflow/03_rbm_initialized_autoencoder.thumbnail.png" src="https://ikhlestov.github.io/images/rbm-based-autoencoders-with-tensorflow/03_rbm_initialized_autoencoder.thumbnail.png"&gt;&lt;/a&gt;
&lt;p class="caption"&gt;Only RBM based autoencoder training process, for clarity&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Also I've trained two autoencoders without Gaussian noise. Now we can see through distribution what embedding most similar to binary (&lt;a class="reference external" href="https://github.com/ikhlestov/rbm_based_autoencoders_with_tensorflow/blob/master/results_validation/visualize_distribution.py"&gt;code for visualization&lt;/a&gt;):&lt;/p&gt;
&lt;div class="figure"&gt;
&lt;a class="reference external image-reference" href="https://ikhlestov.github.io/images/rbm-based-autoencoders-with-tensorflow/04_rbm_aec_embeddings_distribution.png"&gt;&lt;img alt="/images/rbm-based-autoencoders-with-tensorflow/04_rbm_aec_embeddings_distribution.thumbnail.png" src="https://ikhlestov.github.io/images/rbm-based-autoencoders-with-tensorflow/04_rbm_aec_embeddings_distribution.thumbnail.png"&gt;&lt;/a&gt;
&lt;p class="caption"&gt;Comparison of embeddings distributions&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;We can see that RBM based autoencoder with Gaussian noise works better than other for our purposes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="validation"&gt;
&lt;h2&gt;Validation&lt;/h2&gt;
&lt;p&gt;To validate received embeddings I generate them for test and train sets for such networks:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Initial MNIST(without embedding at all)&lt;/li&gt;
&lt;li&gt;RBM with last layer binarized and trained by pairs&lt;/li&gt;
&lt;li&gt;Autoencoder based on RBM with Gaussian noise&lt;/li&gt;
&lt;li&gt;Newly initialized autoencoder with Gaussian noise&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;and use two validation approaches:&lt;/p&gt;
&lt;p&gt;Train SVM with train set and measure accuracy on test set. SVM was used from sklearn with 'rbf' kernel with no &lt;cite&gt;max_iter&lt;/cite&gt; == 50. Results table were generated with &lt;a class="reference external" href="https://github.com/ikhlestov/rbm_based_autoencoders_with_tensorflow/blob/master/results_validation/svm_clusterization_test.py"&gt;this code&lt;/a&gt;&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="56%"&gt;
&lt;col width="11%"&gt;
&lt;col width="11%"&gt;
&lt;col width="11%"&gt;
&lt;col width="11%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;notes&lt;/th&gt;
&lt;th class="head"&gt;accuracy&lt;/th&gt;
&lt;th class="head"&gt;prec&lt;/th&gt;
&lt;th class="head"&gt;f_score&lt;/th&gt;
&lt;th class="head"&gt;recall&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;default mnist dataset&lt;/td&gt;
&lt;td&gt;0.446&lt;/td&gt;
&lt;td&gt;0.647&lt;/td&gt;
&lt;td&gt;0.460&lt;/td&gt;
&lt;td&gt;0.454&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;rbm: train_layers_by_pairs__last_layer_binarized&lt;/td&gt;
&lt;td&gt;0.455&lt;/td&gt;
&lt;td&gt;0.450&lt;/td&gt;
&lt;td&gt;0.446&lt;/td&gt;
&lt;td&gt;0.453&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;autoencoder: rbm_initialized_model__with_Gaussian_noise&lt;/td&gt;
&lt;td&gt;0.499&lt;/td&gt;
&lt;td&gt;0.500&lt;/td&gt;
&lt;td&gt;0.493&lt;/td&gt;
&lt;td&gt;0.494&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;autoencoder: new_initialized_model__with_Gaussian_noise&lt;/td&gt;
&lt;td&gt;0.100&lt;/td&gt;
&lt;td&gt;0.098&lt;/td&gt;
&lt;td&gt;0.095&lt;/td&gt;
&lt;td&gt;0.099&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;With hamming distance or dot product find 10 most similar pictures/embeddings to provided one and check how many labels are the same to the submitted array label. &lt;a class="reference external" href="https://github.com/ikhlestov/rbm_based_autoencoders_with_tensorflow/blob/master/results_validation/found_similiar.py"&gt;Code&lt;/a&gt; to check distance accuracies.&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="56%"&gt;
&lt;col width="11%"&gt;
&lt;col width="11%"&gt;
&lt;col width="11%"&gt;
&lt;col width="11%"&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;notes&lt;/th&gt;
&lt;th class="head"&gt;hamming_accuracy&lt;/th&gt;
&lt;th class="head"&gt;hamming_time_cons&lt;/th&gt;
&lt;th class="head"&gt;dot_product_accuracy&lt;/th&gt;
&lt;th class="head"&gt;dot_product_time_cons&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;default mnist dataset&lt;/td&gt;
&lt;td&gt;0.910&lt;/td&gt;
&lt;td&gt;180.4&lt;/td&gt;
&lt;td&gt;0.916&lt;/td&gt;
&lt;td&gt;528.8&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;rbm: train_layers_by_pairs__last_layer_binarized&lt;/td&gt;
&lt;td&gt;0.633&lt;/td&gt;
&lt;td&gt;28.6&lt;/td&gt;
&lt;td&gt;0.638&lt;/td&gt;
&lt;td&gt;60.2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;autoencoder: rbm_initialized_model__with_Gaussian_noise&lt;/td&gt;
&lt;td&gt;0.583&lt;/td&gt;
&lt;td&gt;28.9&lt;/td&gt;
&lt;td&gt;0.563&lt;/td&gt;
&lt;td&gt;61.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;autoencoder: new_initialized_model__with_Gaussian_noise&lt;/td&gt;
&lt;td&gt;0.099&lt;/td&gt;
&lt;td&gt;29.8&lt;/td&gt;
&lt;td&gt;0.099&lt;/td&gt;
&lt;td&gt;64.6&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;As we can see embeddings can save some strong features, that can be used for future clusterization very well. But this features are not linearly correlated - so when we measure accuracy for most similar embeddings we get results worse than when we use full MNIST images. Of course maybe autoencoder should be trained with another leaning rate/longer but this is task for future research.&lt;/p&gt;
&lt;p&gt;At the same time we confirmed that training autoencoders from pretrained RBMs weights is good approach - network will pass local optimization minimum and not stack at some point during training.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="training-params"&gt;
&lt;h2&gt;Training params&lt;/h2&gt;
&lt;p&gt;For RBM training such params were used network was trained with:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;epochs = 6&lt;/li&gt;
&lt;li&gt;learning rate = 0.01&lt;/li&gt;
&lt;li&gt;batch size = 100&lt;/li&gt;
&lt;li&gt;shuffle batches = True&lt;/li&gt;
&lt;li&gt;gibbs sampling steps = 1&lt;/li&gt;
&lt;li&gt;layers quantity = 3&lt;/li&gt;
&lt;li&gt;layers shapes(including input layer) = [784, 484, 196, 100]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For autoencoder learning rate was changed to 1.0 because another optimization rule.&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;</description><guid>https://ikhlestov.github.io/posts/rbm-based-autoencoders-with-tensorflow/</guid><pubDate>Wed, 28 Dec 2016 20:33:15 GMT</pubDate></item><item><title>Welcome Post</title><link>https://ikhlestov.github.io/posts/welcome-post/</link><dc:creator>Illarion Khlestov</dc:creator><description>&lt;p&gt;Hi to all! There only a few days I've set up this blog and I've decided that there should be at least one post.
The main purpose of this blog to collect and organize some coding related info.
At the &lt;a class="reference external" href="https://ikhlestov.github.io/pages"&gt;Pages&lt;/a&gt;, I will keep lists with useful commands mainly without the long explanation.
At the Blogs, I will post some more verbose things. Will see how it will go.&lt;/p&gt;</description><guid>https://ikhlestov.github.io/posts/welcome-post/</guid><pubDate>Wed, 29 Jun 2016 01:39:27 GMT</pubDate></item></channel></rss>