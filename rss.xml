<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Illarion Khlestov Blog</title><link>https://ikhlestov.github.io/</link><description>Site about coding notes</description><atom:link rel="self" type="application/rss+xml" href="https://ikhlestov.github.io/rss.xml"></atom:link><language>en</language><lastBuildDate>Mon, 09 Jan 2017 23:06:21 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>RBM based Autoencoders with tensorflow</title><link>https://ikhlestov.github.io/posts/rbm-based-autoencoders-with-tensorflow/</link><dc:creator>Illarion Khlestov</dc:creator><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;This is draft of post - feel free to comment/mail me about any errors&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Recently I try to implement RBM based autoencoder in tensorflow similar to RBMs described in &lt;a class="reference external" href="http://www.cs.utoronto.ca/~rsalakhu/papers/semantic_final.pdf"&gt;Semantic Hashing&lt;/a&gt; paper by Ruslan Salakhutdinov and Geoffrey Hinton. It seems that with weights that were pre-trained with RBM autoencoders should converge faster. So I've decided to check this.&lt;/p&gt;
&lt;p&gt;This post will describe some roadblocks for RBMs/autoencoders implementation in tensorflow and compare results of different approaches. I assume reader's previous knowledge of tensorflow and machine learning field. All code can be found in &lt;a class="reference external" href="https://github.com/ikhlestov/rbm_based_autoencoders_with_tensorflow"&gt;this repo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;RBMs different from usual neural networks in some ways:&lt;/p&gt;
&lt;p&gt;Neural networks usually perform weight update by Gradient Descent, but RMBs use &lt;strong&gt;Contrastive Divergence&lt;/strong&gt; (which is basically a funky term for "approximate gradient descent" &lt;a class="reference external" href="http://deeplearning.net/tutorial/rbm.html"&gt;link to read&lt;/a&gt;). At a glance, contrastive divergence computes a difference between &lt;strong&gt;positive phase&lt;/strong&gt; (energy of first encoding) and &lt;strong&gt;negative phase&lt;/strong&gt; (energy of the last encoding).&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_73686ce898d04d768eb8efbd81b91f65-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_73686ce898d04d768eb8efbd81b91f65-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;visib_inputs_initial&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;first_encoded_probability&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_73686ce898d04d768eb8efbd81b91f65-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;negative_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_73686ce898d04d768eb8efbd81b91f65-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;last_reconstructed_probability&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;last_encoded_probability&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_73686ce898d04d768eb8efbd81b91f65-5"&gt;&lt;/a&gt;&lt;span class="n"&gt;contrastive_divergence&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;negative_phase&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Also, a key feature of RMB that it encode output in binary mode, not as probabilities. More about RMBs you may read &lt;a class="reference external" href="http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/"&gt;here&lt;/a&gt; or &lt;a class="reference external" href="http://rocknrollnerd.github.io/ml/2015/07/18/general-boltzmann-machines.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As prototype one layer tensorflow rbm &lt;a class="reference external" href="https://github.com/blackecho/Deep-Learning-TensorFlow/blob/master/yadlt/models/rbm_models/rbm.py"&gt;implementation&lt;/a&gt; was used. For testing, I've taken well known &lt;a class="reference external" href="https://en.wikipedia.org/wiki/MNIST_database"&gt;MNIST&lt;/a&gt; dataset(dataset of handwritten digits).&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ikhlestov.github.io/posts/rbm-based-autoencoders-with-tensorflow/"&gt;Read moreâ€¦&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><guid>https://ikhlestov.github.io/posts/rbm-based-autoencoders-with-tensorflow/</guid><pubDate>Wed, 28 Dec 2016 20:33:15 GMT</pubDate></item><item><title>Welcome Post</title><link>https://ikhlestov.github.io/posts/welcome-post/</link><dc:creator>Illarion Khlestov</dc:creator><description>&lt;p&gt;Hi to all! There only a few days I've set up this blog and I've decided that there should be at least one post.
The main purpose of this blog to collect and organize some coding related info.
At the &lt;a class="reference external" href="https://ikhlestov.github.io/pages"&gt;Pages&lt;/a&gt;, I will keep lists with useful commands mainly without the long explanation.
At the Blogs, I will post some more verbose things. Will see how it will go.&lt;/p&gt;</description><guid>https://ikhlestov.github.io/posts/welcome-post/</guid><pubDate>Wed, 29 Jun 2016 01:39:27 GMT</pubDate></item></channel></rss>