<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns#
article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Architecture vs. Optimization Approaches | Illarion Khlestov Blog</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../../rss.xml">
<link rel="canonical" href="https://ikhlestov.github.io/pages/machine-learning/architecture_vs_optimization_approaches/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: 'center', // Change this to 'center' to center equations.
});
</script><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Illarion Khlestov">
<meta property="og:site_name" content="Illarion Khlestov Blog">
<meta property="og:title" content="Architecture vs. Optimization Approaches">
<meta property="og:url" content="https://ikhlestov.github.io/pages/machine-learning/architecture_vs_optimization_approaches/">
<meta property="og:description" content="On this page I will try distinguish various approaches that now used for building NN.
For example using convolution or LSTM is an architecture approach.
But using batch normalization or not - is optim">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-03-24T17:32:32Z">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ikhlestov.github.io/">

                <span id="blog-title">Illarion Khlestov Blog</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../../">Blog</a>
                </li>
<li>
<a href="../../">Pages</a>
                </li>
<li>
<a href="../../../listings/">Listings</a>
                </li>
<li>
<a href="../../../archive.html">Archive</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<li>
    <a href="index.rst" id="sourcelink">Source</a>
    </li>
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
<article class="post-text storypage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Architecture vs. Optimization Approaches</a></h1>

        
    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>On this page I will try distinguish various approaches that now used for building NN.
For example using convolution or LSTM is an architecture approach.
But using batch normalization or not - is optimization, because it can be added to any network without any architecture changes.</p>
<p>Architecture:</p>
<ul class="simple">
<li>Convs of with various kernels</li>
<li>1x1 convolutions from <a class="reference external" href="https://arxiv.org/abs/1312.4400">Network-in-network(NiN)</a>. With them we may reduce features &gt; perform usual convolution &gt; increase number of features</li>
<li>Average pooling layer as part of the last classifier</li>
<li>Inception module(parallel computation of various filter with 1x1 convs and after concatenating them)</li>
<li>Flattened convolutions(Cx1, 1xC kernels)</li>
<li>Bypassing features over two layers(as in ResNet)</li>
<li>Concatenating features from current layer with features from previous ones(as in DenseNet)</li>
<li>Inception V4 - combine ResNet features propagating approach with Inception module.</li>
<li>Combine Inception Block with DenseNet approach.</li>
<li>
<a class="reference external" href="https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba#.itgibj8dm">Blog post</a> about various Neural Networks for image classification.</li>
<li>
<a class="reference external" href="https://arxiv.org/pdf/1610.02357.pdf">XCeption block</a> with separable convolutions(with or without ReLU after it).</li>
<li>Depthwise separable convolution filters <a class="reference external" href="https://arxiv.org/pdf/1412.5474.pdf">initial paper</a>
</li>
<li>LSTM or GRU cell</li>
<li>attention mechanisms</li>
<li>Various activation functions</li>
<li>Max pooling or average pooling</li>
<li>Use conv with stride without overlaping, not average/max pooling</li>
<li>1x1 convs and then separable by channels 3x3 convs</li>
<li>Separable by channnels 3x3 convs and after 1x1 convs for all features</li>
</ul>
<p>Optimization:</p>
<ul class="simple">
<li>Batch norm</li>
<li>Regularization loss</li>
<li>Various learning rate</li>
<li>Dropout</li>
</ul>
<p>Learning:</p>
<ul class="simple">
<li>Dataset augmentation</li>
<li>Learn network to one image size(224x224) and fine tune after for less epochs to larger size(448x448 for example)</li>
<li>Train image detection network with image classification dataset</li>
</ul>
<p>A systematic evaluation of CNN modules:</p>
<ul class="simple">
<li><a class="reference external" href="https://arxiv.org/pdf/1606.02228.pdf">Link to initial paper</a></li>
<li>use ELU non-linearity without batchnorm or ReLU with it.</li>
<li>apply a learned colorspace transformation of RGB.</li>
<li>use the linear learning rate decay policy.</li>
<li>use a sum of the average and max pooling layers.</li>
<li>use mini-batch size around 128 or 256. If this is too big for your GPU, decrease the learning rate proportionally to the batch size.</li>
<li>use fully-connected layers as convolutional and average the predictions for the final decision.</li>
<li>when investing in increasing training set size, check if a plateau has not been reach.</li>
<li>cleanliness of the data is more important then the size.</li>
<li>if you cannot increase the input image size, reduce the stride in the con- sequent layers, it has roughly the same effect.</li>
<li>if your network has a complex and highly optimized architecture, like e.g. GoogLeNet, be careful with modifications.</li>
</ul>
</div>
    </div>
    
</article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents Â© 2017         <a href="mailto:ikhlestov@gmail.com">Illarion Khlestov</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>          <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script></footer>
</div>
</div>

            <script src="../../../assets/js/all-nocdn.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-92406723-1', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>
