<html prefix="
og: http://ogp.me/ns#
article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<base href="https://ikhlestov.github.io/pages/general-ml-notes/">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>General ML Notes | Illarion Khlestov Blog</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="https://ikhlestov.github.io/pages/general-ml-notes/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: 'left', // Change this to 'center' to center equations.
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Illarion Khlestov">
<meta property="og:site_name" content="Illarion Khlestov Blog">
<meta property="og:title" content="General ML Notes">
<meta property="og:url" content="https://ikhlestov.github.io/pages/general-ml-notes/">
<meta property="og:description" content="This notes based on Neural Networks and Deep Learning
and Coursera ML Courses. They may seems to be some way unstructured, but such structure is useful for me.

Contents:

General Approach
Part I
Eval">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2016-10-02T23:00:05Z">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ikhlestov.github.io/">

                <span id="blog-title">Illarion Khlestov Blog</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../">Blog</a>
                </li>
<li>
<a href="../">Pages</a>
                </li>
<li>
<a href="../../listings/">Listings</a>
                </li>
<li>
<a href="../../categories/">Tags</a>
                </li>
<li>
<a href="../../archive.html">Archive</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<li>
    <a href="index.rst" id="sourcelink">Source</a>
    </li>
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
<article class="storypage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">General ML&nbsp;Notes</a></h1>

        
    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>This notes based on <a class="reference external" href="http://neuralnetworksanddeeplearning.com/index.html">Neural Networks and Deep Learning</a>
and <a class="reference external" href="https://www.coursera.org/learn/machine-learning">Coursera ML Courses</a>. They may seems to be some way unstructured, but such structure is useful for&nbsp;me.</p>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents:</p>
<ul class="simple">
<li><a class="reference internal" href="#general-approach" id="id1">General&nbsp;Approach</a></li>
<li><a class="reference internal" href="#part-i" id="id2">Part&nbsp;I</a></li>
<li><a class="reference internal" href="#evaluation-of-algorithm" id="id3">Evaluation of&nbsp;algorithm</a></li>
<li><a class="reference internal" href="#overfiting-and-underfiting" id="id4">Overfiting and&nbsp;underfiting</a></li>
</ul>
</div>
<div class="section" id="general-approach">
<h2><a class="toc-backref" href="#id1">General&nbsp;Approach</a></h2>
<ul class="simple">
<li>Define network&nbsp;architecture</li>
<li>Choose right cost&nbsp;function</li>
<li>Calculate gradient descent if&nbsp;necessary</li>
<li>Train, tune&nbsp;hyperparameters.</li>
</ul>
</div>
<div class="section" id="part-i">
<h2><a class="toc-backref" href="#id2">Part&nbsp;I</a></h2>
<p>Sigmoid&nbsp;function:</p>
<div class="math">
\begin{equation*}
\sigma(z) = \frac{1}{1 + e^{-z}}
\end{equation*}
</div>
<p><span class="math">\(\sigma(\infty)\approx 1\)</span>, <span class="math">\(\sigma(-\infty)\approx 0\)</span>,
but note, that <span class="math">\(\sigma(0)=1\)</span></p>
<p>Note: <em>sigmoid function</em> (<span class="math">\(\sigma\)</span>) == <em>logistic function</em>
so <em>sigmoid neurons</em> can be called as <em>logistic neurons</em>.</p>
<p><strong>MLP</strong> is an abbreviation for <em>multilayer&nbsp;perceptrons</em></p>
<p><em>cost</em> fucntion == <em>loss</em> function == <em>objective</em>&nbsp;function.</p>
<p><em>Quadratic cost function</em> (or <em>mean squared error</em>, or just <em>MSE</em>):</p>
<div class="math">
\begin{equation*}
C(w,b)  = \frac{1}{2n}\sum_{n}||y(x) - a||^2
\end{equation*}
</div>
<p>Here,
<em>w</em> denotes the collection of all weights in the network,
<em>b</em> all the biases,
<em>n</em> is the total number of training inputs,
<em>a</em> is the vector of outputs from the network when <em>x</em> is input,
and the sum is over all training inputs, <em>x</em>.</p>
<p>An idea of <em>stochastic gradient descent</em> is to estimate the gradient
<span class="math">\(\nabla C\)</span> by computing <span class="math">\(\nabla Cx\)</span> for a small sample of randomly chosen training inputs,
not for all inputs as usual <em>gradient descent</em> do.
For this stochastic gradient descent take small number of <em>m</em> randomly chosen training inputs.
We&#8217;ll label those random training inputs <span class="math">\(X1,X2,\ldots  ,Xm\)</span> and refer to them as a <em>mini-batch</em>.
So now gradinet can be computed&nbsp;as:</p>
<div class="math">
\begin{equation*}
\nabla C \approx \frac{1}{m}\sum_{j=1}^m \nabla C_{X_j}
\end{equation*}
</div>
</div>
<div class="section" id="evaluation-of-algorithm">
<h2><a class="toc-backref" href="#id3">Evaluation of&nbsp;algorithm</a></h2>
<p>What we should&nbsp;do:</p>
<ol class="arabic simple">
<li>Split the dataset into three portions: train set, validate set and test set, in a proportion&nbsp;3:1:1.</li>
<li>When the number of examples <em>m</em> increase, the cost <span class="math">\({J_{test}}\)</span> increases, while <span class="math">\({J_{val}}\)</span> decrease. When <em>m</em> is very large, if <span class="math">\({J_{test}}\)</span> is about equal to <span class="math">\({J_{val}}\)</span> the algorithm may suffer from large bias(underfiting), while if there is a gap between <span class="math">\({J_{test}}\)</span> and <span class="math">\({J_{val}}\)</span> the algorithm may suffer from large&nbsp;variance(overfiting).</li>
<li>To solve the problem of large bias, you may decrease <span class="math">\({\rm{\lambda }}\)</span> in regularization, while increase it for the problem of large&nbsp;variance.</li>
<li>To evaluate the performance of a classification algorithm, we can use the value: precision, recall and&nbsp;F1.</li>
</ol>
<p>Precision:</p>
<div class="math">
\begin{equation*}
\frac{{TruePositive}}{{TruePositive + FalsePositive}}
\end{equation*}
</div>
<p>Recall:</p>
<div class="math">
\begin{equation*}
\frac{{TruePositive}}{{TruePositive + FalseNegtive}}
\end{equation*}
</div>
<p>F1:</p>
<div class="math">
\begin{equation*}
\frac{{2*Recall*Precision}}{{Recall + Precision}}
\end{equation*}
</div>
</div>
<div class="section" id="overfiting-and-underfiting">
<h2><a class="toc-backref" href="#id4">Overfiting and&nbsp;underfiting</a></h2>
<a class="reference external image-reference" href="../../images/ML_notes/bias_and_variance.jpg"><img alt="/images/ML_notes/bias_and_variance.thumbnail.jpg" src="../../images/ML_notes/bias_and_variance.thumbnail.jpg"></a>
<p>For understanding what exactly mean <em>Bias</em> and <em>Variance</em> you may check <a class="reference external" href="http://scott.fortmann-roe.com/docs/BiasVariance.html">this</a>
or <a class="reference external" href="http://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/">this</a>
cool&nbsp;articles.</p>
<p>To deal with them check this articles:
<a class="reference external" href="https://share.coursera.org/wiki/index.php/ML:Advice_for_Applying_Machine_Learning">Advice for Applying Machine Learning</a>,
<a class="reference external" href="https://share.coursera.org/wiki/index.php/ML:Machine_Learning_System_Design">Machine Learning System Design</a>,
<a class="reference external" href="https://share.coursera.org/wiki/index.php/ML:Large_Scale_Machine_Learning">Large Scale Machine Learning</a>.</p>
<p>High <strong>bias</strong> is <strong>underfitting</strong> and high <strong>variance</strong> is <strong>overfitting</strong>.</p>
<p>Our decision process can be broken down as&nbsp;follows:</p>
<ul class="simple">
<li>Fixes high variance(overfiting):<ul>
<li>Getting more training&nbsp;examples</li>
<li>Trying smaller sets of&nbsp;features</li>
</ul>
</li>
<li>Fixes high bias(underfiting):<ul>
<li>Adding&nbsp;features</li>
<li>Adding polynomial&nbsp;features</li>
</ul>
</li>
</ul>
<a class="reference external image-reference" href="../../images/ML_notes/high_variance.png"><img alt="/images/ML_notes/high_variance.thumbnail.png" src="../../images/ML_notes/high_variance.thumbnail.png"></a>
<a class="reference external image-reference" href="../../images/ML_notes/high_bias.png"><img alt="/images/ML_notes/high_bias.thumbnail.png" src="../../images/ML_notes/high_bias.thumbnail.png"></a>
<p>When the hypothesis function is too complex
or there are too many features while the number of training examples is not large enough,
you may get an overfitting problem.
In that case, <span class="math">\(J\left( \theta \right)\)</span> of the training set may be very low,
while that of the validate set and test set can be high.
A good method to solve the problem is regularization which adds the squared
term of parameters to the cost&nbsp;function.</p>
<a class="reference external image-reference" href="../../images/ML_notes/bias_vs_variance_1.png"><img alt="/images/ML_notes/bias_vs_variance_1.thumbnail.png" src="../../images/ML_notes/bias_vs_variance_1.thumbnail.png"></a>
<a class="reference external image-reference" href="../../images/ML_notes/bias_vs_variance_2.png"><img alt="/images/ML_notes/bias_vs_variance_2.thumbnail.png" src="../../images/ML_notes/bias_vs_variance_2.thumbnail.png"></a>
<ul class="simple">
<li>A neural <strong>network with fewer parameters</strong> is prone to <strong>underfitting</strong>. It is also computationally&nbsp;cheaper.</li>
<li>A <strong>large neural network</strong> with more parameters is prone to <strong>overfitting</strong>. It is also computationally&nbsp;expensive.</li>
</ul>
</div>
</div>
    </div>
    
</article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2016         <a href="mailto:ikhlestov@gmail.com">Illarion Khlestov</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>          <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script></footer>
</div>
</div>

            <script src="../../assets/js/all-nocdn.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates -->
</body>
</html>